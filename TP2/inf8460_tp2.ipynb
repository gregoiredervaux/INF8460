{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "Département Génie Informatique et Génie Logiciel\n",
    "INF8460 – Traitement automatique de la langue naturelle\n",
    "\n",
    "### Prof. Amal Zouaq\n",
    "### Chargé de laboratoire: Félix Martel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8460 - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "•\tExplorer les modèles d’espaces vectoriels comme représentations distribuées de la sémantique des mots et des documents\n",
    "\n",
    "•\tComprendre différentes mesures de distance entre vecteurs de documents et de mots\n",
    "\n",
    "•\tUtiliser un modèle de langue n-gramme de caractères et l’algorithme Naive Bayes pour l’analyse de sentiments dans des revues de films (positives, négatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données est séparé en deux répertoires `train/`et `test`, chacun contenant eux-mêmes deux sous-répertoires `pos/` et `neg/` pour les revues positives et négatives. Un fichier `readme` décrit plus précisément les données.\n",
    "\n",
    "Commencez par lire ces données, en gardant séparées les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "global dictionnaire\n",
    "dictionnaire = {\n",
    "    \"test\": {\"pos\": {},\n",
    "            \"neg\": {}},\n",
    "    \"train\": {\"pos\": {},\n",
    "            \"neg\": {}}\n",
    "}\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        path = './data/' + train_type + '/' + classification + '/'\n",
    "        for file in os.listdir(path):\n",
    "            id_review, rate_review = file.split(\"_\")\n",
    "            rate_review = rate_review.split(\".txt\")[0]\n",
    "            if int(id_review) not in dictionnaire[train_type][classification]:\n",
    "                with open(path + file, \"r\") as f:\n",
    "                    dictionnaire[train_type][classification][int(id_review)] = { \"rate\": int(rate_review),\n",
    "                                                                            \"review\": f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Créez la fonction `clean_doc()` qui effectue les pré-traitements suivants : segmentation en mots ; \n",
    "suppression des signes de ponctuations ; suppression des mots qui contiennent des caractères autres qu’alphabétiques ; \n",
    "suppression des mots qui sont connus comme des stop words ; suppression des mots qui ont une longueur de 1 caractère. Ensuite, appliquez-la à vos données.\n",
    "\n",
    "Les stop words peuvent être obtenus avec `from nltk.corpus import stopwords`. Vous pourrez utiliser des [expressions régulières](https://docs.python.org/3.7/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went', 'saw', 'movie', 'last', 'night', 'coaxed', 'friends', 'mine', 'admit', 'reluctant', 'see', 'knew', 'Ashton', 'Kutcher', 'able', 'comedy', 'wrong', 'Kutcher', 'played', 'character', 'Jake', 'Fischer', 'well', 'Kevin', 'Costner', 'played', 'Ben', 'Randall', 'professionalism', 'The', 'sign', 'good', 'movie', 'toy', 'emotions', 'This', 'one', 'exactly', 'The', 'entire', 'theater', 'sold', 'overcome', 'laughter', 'first', 'half', 'movie', 'moved', 'tears', 'second', 'half', 'While', 'exiting', 'theater', 'saw', 'many', 'women', 'tears', 'many', 'full', 'grown', 'men', 'well', 'trying', 'desperately', 'let', 'anyone', 'see', 'crying', 'This', 'movie', 'great', 'suggest', 'go', 'see', 'judge']\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(dictio):\n",
    "\n",
    "    for type_dataset in dictio:\n",
    "        for sentiment_type in dictio[type_dataset]:\n",
    "            for id_review in dictio[type_dataset][sentiment_type]:\n",
    "                review = dictio[type_dataset][sentiment_type][id_review][\"review\"]\n",
    "                review = review.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./?\\|`~-=_+\"})\n",
    "                review = review.split()\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                review_list = [w for w in review if not w in stop_words]\n",
    "                segmentize_review = []\n",
    "\n",
    "                for word in review_list:\n",
    "                    if(re.match('^[a-zA-Z]+$', word) and len(word)>1):\n",
    "                        segmentize_review.append(word)\n",
    "\n",
    "                dictio[type_dataset][sentiment_type][id_review][\"review\"] = segmentize_review\n",
    "\n",
    "\n",
    "\n",
    "clean_doc(dictionnaire)\n",
    "print(dictionnaire[\"test\"][\"pos\"][0][\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tCréez la fonction `build_voc()` qui extrait les unigrammes de l’ensemble d’entraînement et conserve ceux qui ont une fréquence d’occurrence de 5 au moins et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots dans le vocabulaire : 31558\n"
     ]
    }
   ],
   "source": [
    "def build_voc(dico_train):\n",
    "    count = collections.defaultdict(lambda: 0)\n",
    "    for classi in dico_train:\n",
    "        for review in dico_train[classi]:\n",
    "            text = dico_train[classi][review][\"review\"]\n",
    "            for word in text:\n",
    "                if word in count:\n",
    "                    count[word] += 1\n",
    "                else:\n",
    "                    count[word] = 1\n",
    "    n = 0\n",
    "    f = open(\"./data/vocab.txt\", \"w+\")\n",
    "    for word in count:\n",
    "        if count[word] >= 5:\n",
    "            n += 1\n",
    "            f.write(word + \"\\n\")\n",
    "    print(\"Nombre de mots dans le vocabulaire : \" + str(n))\n",
    "    f.close()\n",
    "    return n\n",
    "\n",
    "taille_voc = build_voc(dictionnaire[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Vous devez créer une fonction `get_top_unigrams(n)` qui retourne les $n$ unigrammes les plus fréquents et les affiche, puis l'appeler avec $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams(n):\n",
    "\n",
    "    dico_count = {}\n",
    "\n",
    "    for train_type in [\"test\", \"train\"]:\n",
    "        for classification in [\"pos\", \"neg\"]:\n",
    "            for review in dictionnaire[train_type][classification]:\n",
    "                for word in dictionnaire[train_type][classification][review]['review']:\n",
    "                    if word not in dico_count:\n",
    "                        dico_count[word] = 1\n",
    "                    else:\n",
    "                        dico_count[word] += 1\n",
    "\n",
    "    counter = collections.Counter(dico_count)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\tVous devez créer une fonction `get_top_unigrams_per_cls(n, cls)` qui retourne les $n$ unigrammes les plus fréquents de la classe `cls` (pos ou neg) et les affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams_per_cls(n, cls):\n",
    "    dico_count = {}\n",
    "\n",
    "    for train_type in [\"test\", \"train\"]:\n",
    "        for review in dictionnaire[train_type][cls]:\n",
    "            for word in dictionnaire[train_type][cls][review]['review']:\n",
    "                if word not in dico_count:\n",
    "                    dico_count[word] = 1\n",
    "                else:\n",
    "                    dico_count[word] += 1\n",
    "\n",
    "    counter = collections.Counter(dico_count)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)**\tAffichez les 10 unigrammes les plus fréquents dans la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'The', 'one', 'like', 'good', 'This', 'time', 'story', 'great']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_unigrams_per_cls(10, \"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)**\tAffichez les 10 unigrammes les plus fréquents dans la classe négative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'film', 'The', 'one', 'like', 'good', 'bad', 'would', 'even', 'This']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_unigrams_per_cls(10, \"neg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrices de co-occurence\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Matrice document × mot et TF-IDF\n",
    "\n",
    "\n",
    "Soit $X \\in \\mathbb{R}^{m \\times n}$ une matrice de $m$ documents et $n$ mots, telle que $X_{i,j}$ contient la fréquence d'occurrence du terme $j$ dans le document $i$ :\n",
    "\n",
    "$$\\textbf{rowsum}(X, d) = \\sum_{j=1}^{n}X_{dj}$$\n",
    "\n",
    "$$\\textbf{TF}(X, d, t) = \\frac{X_{d,t}}{\\textbf{rowsum}(X, d)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, t) = \\log\\left(\\frac{m}{|\\{d : X_{d,t} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, d, t) = \\textbf{TF}(X, d, t) \\cdot \\textbf{IDF}(X, t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le même vocabulaire de 5 000 unigrammes, vous devez représenter les documents dans une matrice de co-occurrence document × mot $M(d, w)$  et les pondérer avec la mesure TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les 5000 premiers unigrams\n",
      "['movie', 'film', 'The', 'one', 'like', 'good', 'This', 'time', 'would', 'really', 'story', 'see', 'It', 'even', 'much', 'well', 'get', 'bad', 'people', 'great', 'made', 'first', 'make', 'also', 'way', 'could', 'movies', 'think', 'characters', 'films']\n",
      "\n",
      "Bag of Word\n",
      "fait en : 244.3870131969452s\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 3. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Bag of Word TFIDF\n",
      "fait en : 317.1843571662903s\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.45532785 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.60469684 0.         ... 0.         0.         0.        ]\n",
      " [0.50653484 0.60469684 1.36598354 ... 0.         0.         0.        ]\n",
      " [0.50653484 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "unigrams = get_top_unigrams(5000)\n",
    "print(\"les 5000 premiers unigrams\")\n",
    "print(unigrams[0:30])\n",
    "debut_mm = time.time()\n",
    "BoW = np.array([])\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        # on construit un petit BoW, pur le type et la classe des documents\n",
    "        # pour eviter les appends a chaque nouvelle ligne et profiter des types numpy\n",
    "        BoW_part = np.zeros((len(dictionnaire[train_type][classification]), len(unigrams)))\n",
    "        for index, review in enumerate(dictionnaire[train_type][classification]):\n",
    "            for word in dictionnaire[train_type][classification][review]['review']:\n",
    "                if word in unigrams:\n",
    "                    BoW_part[index, unigrams.index(word)] += 1\n",
    "\n",
    "        BoW = np.append(BoW, BoW_part, axis=0) if BoW.size != 0 else BoW_part\n",
    "\n",
    "print(\"\\nBag of Word\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(BoW)\n",
    "\n",
    "debut_mm = time.time()\n",
    "TFIDF_BoW = BoW.copy()\n",
    "# liste des idf par index (cf unigrams)\n",
    "idf = []\n",
    "for i,_ in enumerate(unigrams):\n",
    "    dfi = 0\n",
    "    for j in range(TFIDF_BoW.shape[0]):\n",
    "        # on compte le nombre de documents qui contiennent ce mot\n",
    "        dfi += 1 if TFIDF_BoW[j, i] != 0 else 0\n",
    "    # on calcul et ajoute le idf a la liste des idf\n",
    "    idf.append(math.log(TFIDF_BoW.shape[0] / dfi))\n",
    "\n",
    "# on modifie la matrice\n",
    "for j in range(TFIDF_BoW.shape[0]):\n",
    "    for i, _ in enumerate(unigrams):\n",
    "        value = TFIDF_BoW[j, i] * idf[i]\n",
    "        TFIDF_BoW[j, i] = value\n",
    "print(\"\\nBag of Word TFIDF\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(TFIDF_BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matrice mot × mot et PPMI (*positive pointwise mutual information*)\n",
    "\n",
    "Vous devez calculer la métrique PPMI. Pour une matrice $m \\times n$ $X$ :\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**\tA partir des textes du corpus d’entrainement (neg *et* pos), vous devez construire une matrice de co-occurrence mot × mot $M(w,w)$ qui contient les 5000 unigrammes les plus fréquents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice_MM\n",
      "fait en : 270.12484216690063s\n",
      "[[    0. 15033. 19527. ...    87.    73.    66.]\n",
      " [15033.     0. 18600. ...    96.   109.    72.]\n",
      " [19527. 18600.     0. ...   121.   112.    93.]\n",
      " ...\n",
      " [   87.    96.   121. ...     0.     0.     0.]\n",
      " [   73.   109.   112. ...     0.     0.     0.]\n",
      " [   66.    72.    93. ...     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "matrice_MM = np.zeros((len(unigrams), len(unigrams)))\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        for review in dictionnaire[train_type][classification]:\n",
    "            # on selectionne seulement les mots qui font parti des 5000 selectionnés\n",
    "            words_to_consider  = list(set(unigrams).intersection(dictionnaire[train_type][classification][review]['review']))\n",
    "            index_to_consider = [unigrams.index(couple_word) for couple_word in words_to_consider]\n",
    "            permutation = itertools.permutations(index_to_consider, 2)\n",
    "            for index_to_consider in permutation:\n",
    "                matrice_MM[index_to_consider] += 1\n",
    "\n",
    "print(\"matrice_MM\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(matrice_MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tVous devez créer une fonction `calculate_PPMI` qui prend la matrice $M(w,w)$ et la transforme en une matrice $M’(w,w)$ avec les valeurs PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice_PPMI\n",
      "fait en : 36.16415572166443s\n",
      "[[0.         0.09212505 0.20809633 ... 0.         0.         0.        ]\n",
      " [0.09212505 0.         0.14769804 ... 0.         0.         0.        ]\n",
      " [0.20809633 0.14769804 0.         ... 0.00283198 0.         0.07636374]\n",
      " ...\n",
      " [0.         0.         0.00283198 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.07636374 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "def calculate_PPMI(matrice_MM):\n",
    "    colsum = np.sum(matrice_MM, axis=0)\n",
    "    row_sum = np.sum(matrice_MM, axis=1)\n",
    "    sum = np.sum(matrice_MM)\n",
    "    expected = np.zeros((matrice_MM.shape[0], matrice_MM.shape[1]))\n",
    "    ppmi =  np.zeros((matrice_MM.shape[0], matrice_MM.shape[1]))\n",
    "    for index,_ in np.ndenumerate(expected):\n",
    "        expected[index] = (row_sum[index[0]] * colsum[index[1]]) / sum\n",
    "        pmi = math.log(matrice_MM[index] / expected[index]) if matrice_MM[index] > 0 else 0\n",
    "        ppmi[index] = pmi if pmi > 0 else 0\n",
    "\n",
    "    return ppmi\n",
    "\n",
    "print(\"matrice_PPMI\")\n",
    "matrice_PPMI = calculate_PPMI(matrice_MM)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(matrice_PPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mesures de similarité\n",
    "\n",
    "En utilisant le module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html),  définissez des fonctions pour calculer les métriques suivantes :\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond à la longueur de la ligne droite entre deux points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Implémentez la fonction `get_euclidean_distance(v1 ,v2)` qui retourne la distance euclidienne entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance(v1 ,v2):\n",
    "    return distance.euclidean(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'écrit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarité cosinus* entre $u$ et $v$.\n",
    "\n",
    "**b)** Implémentez la fonction `get_cosinus_distance(v1, v2)` qui retourne la distance cosinus entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosinus_distance(v1, v2):\n",
    "    return distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Implémentez la fonction `get_most_similar_PPMI(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M’(w,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_PPMI(word, metric, n):\n",
    "    index_word = unigrams.index(word)\n",
    "    ligne_word = matrice_PPMI[index_word]\n",
    "    vector_distance = {}\n",
    "    i = 0\n",
    "    if metric == \"cosinus\":\n",
    "        for index, ligne in enumerate(matrice_PPMI):\n",
    "            vector_distance[unigrams[index]] = get_cosinus_distance(ligne_word, ligne)\n",
    "            i += 1\n",
    "    elif metric == \"euclidean\":\n",
    "        for index, ligne in enumerate(matrice_PPMI):\n",
    "            vector_distance[unigrams[index]] = get_euclidean_distance(ligne_word, ligne)\n",
    "            i += 1\n",
    "    else:\n",
    "        print(\"metric inconnue\")\n",
    "\n",
    "    counter = collections.Counter(vector_distance)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les mots les plus proches de \"bad\": \n",
      "['Custer', 'Streisand', 'Cagney', 'Bugs', 'Felix', 'Garfield', 'Karloff', 'Columbo', 'Barney', 'Frost', 'Matthau', 'Lincoln', 'Cinderella', 'Hart', 'Radio']\n",
      "fait en : 0.12471961975097656s\n",
      "les mots les plus proches de \"bad\": \n",
      "['beautifully', 'beautiful', 'young', 'war', 'War', 'marriage', 'emotional', 'relationship', 'lives', 'journey', 'father', 'life', 'beauty', 'mother', 'wonderful']\n",
      "fait en : 0.2219839096069336s\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_PPMI(\"bad\", \"euclidean\", 15)\n",
    "print(most_similar)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "\n",
    "debut_mm = time.time()\n",
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_PPMI(\"bad\", \"cosinus\", 15)\n",
    "print(most_similar)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici<-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Implémentez la fonction `get_most_similar_TFIDF(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M(d,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_TFIDF(word, metric, n):\n",
    "    index_word = unigrams.index(word)\n",
    "    column_word = TFIDF_BoW[:, index_word]\n",
    "    vector_distance = {}\n",
    "    if metric == \"cosinus\":\n",
    "        for index, column in enumerate(TFIDF_BoW.T):\n",
    "            vector_distance[unigrams[index]] = get_cosinus_distance(column_word, column)\n",
    "    elif metric == \"euclidean\":\n",
    "        for index, column in enumerate(TFIDF_BoW.T):\n",
    "            vector_distance[unigrams[index]] = get_euclidean_distance(column_word, column)\n",
    "    else:\n",
    "        print(\"metric inconnue\")\n",
    "\n",
    "    counter = collections.Counter(vector_distance)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les mots les plus proches de \"bad\": \n",
      "['show', 'game', 'match', 'Batman', 'series', 'book', 'Custer', 'Bond', 'Holmes', 'war', 'THE', 'horror', 'film', 'episode', 'Tarzan']\n",
      "fait en : 11.153342247009277s\n",
      "les mots les plus proches de \"bad\": \n",
      "['Felix', 'Cassavetes', 'Bugs', 'Scarlett', 'Custer', 'Matthau', 'Lemmon', 'Heart', 'Streisand', 'Garfield', 'Stanwyck', 'Bourne', 'Jesse', 'Le', 'Holly']\n",
      "fait en : 11.430819749832153s\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_TFIDF(\"bad\", \"euclidean\", 15)\n",
    "print(most_similar)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "\n",
    "debut_mm = time.time()\n",
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_TFIDF(\"bad\", \"cosinus\", 15)\n",
    "print(most_similar)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici <-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification de documents avec un modèle de langue\n",
    "\n",
    "En vous inspirant de [cet article](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139), entraînez deux modèles de langue $n$-gramme de caractère avec lissage de Laplace, l'un sur le corpus `pos`, l'autre sur le corpus `neg`. Puis, pour chaque document $D$, calculez sa probabilité selon vos deux modèles : $P(D \\mid \\textrm{pos})$ et $P(D \\mid \\textrm{neg})$.\n",
    "\n",
    "Vous pourrez alors prédire sa classe $\\hat{c}_D \\in (\\textrm{pos}, \\textrm{neg})$ en prenant :\n",
    "\n",
    "$$\\hat{c}_D = \\begin{cases}\n",
    "\\textrm{pos} & \\textrm{si } P(D \\mid \\textrm{pos}) > P(D \\mid \\textrm{neg}) \\\\\n",
    "\\textrm{neg} & \\textrm{sinon}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(test positif | lm_pos): -2185.1798168179816\n",
      "P(test positif | lm_neg): -2226.6177662434015\n",
      "P(test negatif | lm_pos): -16419.940324263607\n",
      "P(test negatif | lm_neg): -16292.85246181353\n",
      "\n",
      "nombre de positifs: 8049\n",
      "nombre de négatifs: 4451\n"
     ]
    }
   ],
   "source": [
    "from collections import *\n",
    "\n",
    "def train_words_lm(documents_lists, order=3):\n",
    "    lm = defaultdict(Counter)\n",
    "    for review_index in documents_lists:\n",
    "        review = documents_lists[review_index][\"review\"]\n",
    "        for i in range(len(review)-order):\n",
    "            history, word = review[i:i+order], review[i+order]\n",
    "            lm[tuple(history)][word]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        unsmooth = {}\n",
    "        for c,cnt in counter.items():\n",
    "            unsmooth[c] = math.log((cnt + 1)/(s + taille_voc))\n",
    "        unsmooth[\"not_found\"] = math.log(1/(s + taille_voc))\n",
    "        return unsmooth\n",
    "    outlm = {hist:normalize(words) for hist, words in lm.items()}\n",
    "    outlm[\"not_found\"] = math.log(1 / int(taille_voc)**order)\n",
    "    return outlm\n",
    "\n",
    "lm_pos = train_words_lm(dictionnaire[\"train\"][\"pos\"])\n",
    "lm_neg = train_words_lm(dictionnaire[\"train\"][\"neg\"])\n",
    "\n",
    "def calcul_proba_doc(document, lm, order=3):\n",
    "    proba = 0\n",
    "    for i in range(len(document) - order):\n",
    "        history, word = tuple(document[i:i+order]), document[i+order]\n",
    "        if history in lm:\n",
    "            if word in lm[history]:\n",
    "                proba += lm[history][word] \n",
    "            else:\n",
    "                proba += lm[history][\"not_found\"]\n",
    "        else:\n",
    "            proba += lm[\"not_found\"]\n",
    "    return proba\n",
    "\n",
    "print(\"P(test positif | lm_pos): \" + str(calcul_proba_doc(dictionnaire[\"test\"][\"pos\"][10][\"review\"], lm_pos)))\n",
    "print(\"P(test positif | lm_neg): \" + str(calcul_proba_doc(dictionnaire[\"test\"][\"pos\"][10][\"review\"], lm_neg)))\n",
    "print(\"P(test negatif | lm_pos): \" + str(calcul_proba_doc(dictionnaire[\"test\"][\"neg\"][10][\"review\"], lm_pos)))\n",
    "print(\"P(test negatif | lm_neg): \" + str(calcul_proba_doc(dictionnaire[\"test\"][\"neg\"][10][\"review\"], lm_neg)))\n",
    "\n",
    "class_by_index = []\n",
    "nb_pos = 0\n",
    "for index_doc in range(len(dictionnaire[\"test\"][\"pos\"])):\n",
    "    proba_pos = calcul_proba_doc(dictionnaire[\"test\"][\"pos\"][index_doc][\"review\"], lm_pos)\n",
    "    proba_neg = calcul_proba_doc(dictionnaire[\"test\"][\"pos\"][index_doc][\"review\"], lm_neg)\n",
    "    if proba_pos > proba_neg:\n",
    "        nb_pos += 1\n",
    "        class_by_index.append(\"pos\")\n",
    "    else:\n",
    "        class_by_index.append(\"neg\")\n",
    "        \n",
    "print(\"\\nnombre de positifs: \" + str(nb_pos))\n",
    "print(\"nombre de négatifs: \" + str(len(class_by_index) - nb_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification de documents avec sac de mots et Naive Bayes\n",
    "\n",
    "Ici, vous utiliserez l'algorithme Multinomial Naive Bayes (disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) pour classifier les documents. Vous utiliserez un modèle sac de mots (en anglais *bag of words*, ou BoW) avec TF-IDF pour représenter vos documents.\n",
    "\n",
    "*Note :* vous avez déjà construit la matrice TF-IDF à la section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000,)\n",
      "(25000, 5000)\n",
      "(25000,)\n",
      "[0 0 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# y = 0:pos 1:neg\n",
    "X_test = TFIDF_BoW[0:25000][:]\n",
    "y_test_nb = np.full((1, 12500), 0)\n",
    "y_test_nb = np.append(y_test_nb, np.full((1, 12500), 1))\n",
    "\n",
    "X_train = TFIDF_BoW[25000:][:]\n",
    "y_train = np.full((1, 12500), 0)\n",
    "y_train = np.append(y_train, np.full((1, 12500), 1))\n",
    "            \n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_nb = classifier.predict(X_test)\n",
    "print(y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Améliorations\n",
    "\n",
    "Ici, vous devez proposer une méthode d'amélioration pour le modèle précédent, la justifier et l'implémenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Écrivez vos explications ici <-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation\n",
    "\n",
    "Évaluation des modèles des sections 4, 5, 6 sur les données de test. On attend les métriques suivantes : *accuracy*, et pour chaque classe précision, rappel, score F1. Vous pourrez utiliser le module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy NB:\n",
      "0.83368\n",
      "Precision NB:\n",
      "0.818008539188777\n",
      "Recall NB:\n",
      "0.85832\n",
      "F1-score NB:\n",
      "0.837679575265459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy NB:\")\n",
    "print(accuracy_score(y_test_nb, y_pred_nb))\n",
    "\n",
    "# precision\n",
    "print(\"Precision NB:\")\n",
    "print(precision_score(y_test_nb, y_pred_nb, average='binary'))\n",
    "\n",
    "# recall\n",
    "print(\"Recall NB:\")\n",
    "print(recall_score(y_test_nb, y_pred_nb, average='binary'))\n",
    "\n",
    "# F1-score\n",
    "print(\"F1-score NB:\")\n",
    "print(f1_score(y_test_nb, y_pred_nb, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentez vos résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici vos résultats <-*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
