{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# École Polytechnique de Montréal\n",
    "Département Génie Informatique et Génie Logiciel\n",
    "INF8460 – Traitement automatique de la langue naturelle\n",
    "\n",
    "### Prof. Amal Zouaq\n",
    "### Chargé de laboratoire: Félix Martel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8460 - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "•\tExplorer les modèles d’espaces vectoriels comme représentations distribuées de la sémantique des mots et des documents\n",
    "\n",
    "•\tComprendre différentes mesures de distance entre vecteurs de documents et de mots\n",
    "\n",
    "•\tUtiliser un modèle de langue n-gramme de caractères et l’algorithme Naive Bayes pour l’analyse de sentiments dans des revues de films (positives, négatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données est séparé en deux répertoires `train/`et `test`, chacun contenant eux-mêmes deux sous-répertoires `pos/` et `neg/` pour les revues positives et négatives. Un fichier `readme` décrit plus précisément les données.\n",
    "\n",
    "Commencez par lire ces données, en gardant séparées les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers non-ouverts : 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "global dictionnaire\n",
    "# notre dictionnaire de donnée aura cette forme là\n",
    "dictionnaire = {\n",
    "    \"test\": {\"pos\": {},\n",
    "            \"neg\": {}},\n",
    "    \"train\": {\"pos\": {},\n",
    "            \"neg\": {}}\n",
    "}\n",
    "\n",
    "nb_failed_files = 0\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        path = './data/' + train_type + '/' + classification + '/'\n",
    "        for file in os.listdir(path):\n",
    "            id_review, rate_review = file.split(\"_\")\n",
    "            rate_review = rate_review.split(\".txt\")[0]\n",
    "            if int(id_review) not in dictionnaire[train_type][classification]:\n",
    "                with open(path + file, \"r\") as f:\n",
    "                    try:\n",
    "                        dictionnaire[train_type][classification][int(id_review)] = { \"rate\": int(rate_review), \"review_str\": f.read()}\n",
    "                    except:\n",
    "                        nb_failed_files += 1\n",
    "\n",
    "print(\"Nombre de fichiers non-ouverts : \" + str(nb_failed_files))\n",
    "init_dictionnaire = copy.deepcopy(dictionnaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Créez la fonction `clean_doc()` qui effectue les pré-traitements suivants : segmentation en mots ; \n",
    "suppression des signes de ponctuations ; suppression des mots qui contiennent des caractères autres qu’alphabétiques ; \n",
    "suppression des mots qui sont connus comme des stop words ; suppression des mots qui ont une longueur de 1 caractère. Ensuite, appliquez-la à vos données.\n",
    "\n",
    "Les stop words peuvent être obtenus avec `from nltk.corpus import stopwords`. Vous pourrez utiliser des [expressions régulières](https://docs.python.org/3.7/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple: \n",
      "\n",
      "['went', 'saw', 'movie', 'last', 'night', 'coaxed', 'friends', 'mine', 'admit', 'reluctant', 'see', 'knew', 'Ashton', 'Kutcher', 'able', 'comedy', 'wrong', 'Kutcher', 'played', 'character', 'Jake', 'Fischer', 'well', 'Kevin', 'Costner', 'played', 'Ben', 'Randall', 'professionalism', 'The', 'sign', 'good', 'movie', 'toy', 'emotions', 'This', 'one', 'exactly', 'The', 'entire', 'theater', 'sold', 'overcome', 'laughter', 'first', 'half', 'movie', 'moved', 'tears', 'second', 'half', 'While', 'exiting', 'theater', 'saw', 'many', 'women', 'tears', 'many', 'full', 'grown', 'men', 'well', 'trying', 'desperately', 'let', 'anyone', 'see', 'crying', 'This', 'movie', 'great', 'suggest', 'go', 'see', 'judge']\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(dictio):\n",
    "\n",
    "    for type_dataset in dictio:\n",
    "        for sentiment_type in dictio[type_dataset]:\n",
    "            for id_review in dictio[type_dataset][sentiment_type]:\n",
    "                # on va d'abord filtrer les caractères spéciaux\n",
    "                review = dictio[type_dataset][sentiment_type][id_review][\"review_str\"]\n",
    "                review = review.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./?\\|`~-=_+\"})\n",
    "                # on sépare les mots\n",
    "                review = review.split()\n",
    "                # on selectionne les mots qui ne sont pas des stop words\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                review_list = [w for w in review if not w in stop_words]\n",
    "                segmentize_review = []\n",
    "\n",
    "                for word in review_list:\n",
    "                    if(re.match('^[a-zA-Z]+$', word) and len(word)>1):\n",
    "                        segmentize_review.append(word)\n",
    "\n",
    "                dictio[type_dataset][sentiment_type][id_review][\"review\"] = segmentize_review\n",
    "\n",
    "\n",
    "\n",
    "clean_doc(dictionnaire)\n",
    "print(\"exemple: \\n\")\n",
    "print(dictionnaire[\"test\"][\"pos\"][0][\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tCréez la fonction `build_voc()` qui extrait les unigrammes de l’ensemble d’entraînement et conserve ceux qui ont une fréquence d’occurrence de 5 au moins et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots dans le vocabulaire : 31558\n"
     ]
    }
   ],
   "source": [
    "def build_voc(dico_train):\n",
    "    count = collections.defaultdict(lambda: 0)\n",
    "    for classi in dico_train:\n",
    "        for review in dico_train[classi]:\n",
    "            text = dico_train[classi][review][\"review\"]\n",
    "            for word in text:\n",
    "                if word in count:\n",
    "                    count[word] += 1\n",
    "                else:\n",
    "                    count[word] = 1\n",
    "    n = 0\n",
    "    f = open(\"./data/vocab.txt\", \"w+\")\n",
    "    for word in count:\n",
    "        if count[word] >= 5:\n",
    "            n += 1\n",
    "            f.write(word + \"\\n\")\n",
    "    print(\"Nombre de mots dans le vocabulaire : \" + str(n))\n",
    "    f.close()\n",
    "    return n\n",
    "\n",
    "taille_voc = build_voc(dictionnaire[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Vous devez créer une fonction `get_top_unigrams(n)` qui retourne les $n$ unigrammes les plus fréquents et les affiche, puis l'appeler avec $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams(n):\n",
    "\n",
    "    dico_count = {}\n",
    "\n",
    "    for train_type in [\"test\", \"train\"]:\n",
    "        for classification in [\"pos\", \"neg\"]:\n",
    "            for review in dictionnaire[train_type][classification]:\n",
    "                for word in dictionnaire[train_type][classification][review]['review']:\n",
    "                    if word not in dico_count:\n",
    "                        dico_count[word] = 1\n",
    "                    else:\n",
    "                        dico_count[word] += 1\n",
    "    \n",
    "    # on se base sur un counter, bien plus rapide\n",
    "    counter = collections.Counter(dico_count)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\tVous devez créer une fonction `get_top_unigrams_per_cls(n, cls)` qui retourne les $n$ unigrammes les plus fréquents de la classe `cls` (pos ou neg) et les affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams_per_cls(n, cls):\n",
    "    dico_count = {}\n",
    "\n",
    "    for train_type in [\"test\", \"train\"]:\n",
    "        for review in dictionnaire[train_type][cls]:\n",
    "            for word in dictionnaire[train_type][cls][review]['review']:\n",
    "                if word not in dico_count:\n",
    "                    dico_count[word] = 1\n",
    "                else:\n",
    "                    dico_count[word] += 1\n",
    "    # idem\n",
    "    counter = collections.Counter(dico_count)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)**\tAffichez les 10 unigrammes les plus fréquents dans la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'The', 'one', 'like', 'good', 'This', 'time', 'story', 'great']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_unigrams_per_cls(10, \"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)**\tAffichez les 10 unigrammes les plus fréquents dans la classe négative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'film', 'The', 'one', 'like', 'good', 'bad', 'would', 'even', 'This']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_unigrams_per_cls(10, \"neg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrices de co-occurence\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Matrice document × mot et TF-IDF\n",
    "\n",
    "\n",
    "Soit $X \\in \\mathbb{R}^{m \\times n}$ une matrice de $m$ documents et $n$ mots, telle que $X_{i,j}$ contient la fréquence d'occurrence du terme $j$ dans le document $i$ :\n",
    "\n",
    "$$\\textbf{rowsum}(X, d) = \\sum_{j=1}^{n}X_{dj}$$\n",
    "\n",
    "$$\\textbf{TF}(X, d, t) = \\frac{X_{d,t}}{\\textbf{rowsum}(X, d)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, t) = \\log\\left(\\frac{m}{|\\{d : X_{d,t} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, d, t) = \\textbf{TF}(X, d, t) \\cdot \\textbf{IDF}(X, t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le même vocabulaire de 5 000 unigrammes, vous devez représenter les documents dans une matrice de co-occurrence document × mot $M(d, w)$  et les pondérer avec la mesure TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les 5000 premiers unigrams\n",
      "['movie', 'film', 'The', 'one', 'like', 'good', 'This', 'time', 'would', 'really', 'story', 'see', 'It', 'even', 'much', 'well', 'get', 'bad', 'people', 'great', 'made', 'first', 'make', 'also', 'way', 'could', 'movies', 'think', 'characters', 'films']\n",
      "\n",
      "Bag of Word\n",
      "fait en : 179.48207807540894s\n",
      "[[6. 4. 1. ... 0. 0. 0.]\n",
      " [2. 0. 5. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 4. 0. ... 0. 0. 0.]\n",
      " [3. 0. 0. ... 0. 0. 0.]\n",
      " [0. 2. 2. ... 0. 0. 0.]]\n",
      "\n",
      "Bag of Word TFIDF\n",
      "fait en : 235.26119875907898s\n",
      "[[3.03920905 2.41878736 0.45532785 ... 0.         0.         0.        ]\n",
      " [1.01306968 0.         2.27663924 ... 0.         0.         0.        ]\n",
      " [0.         1.20939368 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.50653484 2.41878736 0.         ... 0.         0.         0.        ]\n",
      " [1.51960453 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         1.20939368 0.91065569 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "unigrams = get_top_unigrams(5000)\n",
    "print(\"les 5000 premiers unigrams\")\n",
    "print(unigrams[0:30])\n",
    "debut_mm = time.time()\n",
    "BoW = np.array([])\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        # on construit un BoW vide, pour chaque classe et type\n",
    "        # pour éviter les appends à chaque nouvelle ligne et profiter des types numpy\n",
    "        BoW_part = np.zeros((len(dictionnaire[train_type][classification]), len(unigrams)))\n",
    "        for index, review in enumerate(dictionnaire[train_type][classification]):\n",
    "            for word in dictionnaire[train_type][classification][review]['review']:\n",
    "                if word in unigrams:\n",
    "                    BoW_part[index, unigrams.index(word)] += 1\n",
    "        # on construit la grande matrice à partir des 4 précédentes\n",
    "        BoW = np.append(BoW, BoW_part, axis=0) if BoW.size != 0 else BoW_part\n",
    "\n",
    "print(\"\\nBag of Word\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(BoW)\n",
    "\n",
    "debut_mm = time.time()\n",
    "TFIDF_BoW = BoW.copy()\n",
    "# liste des idf par index (cf unigrams)\n",
    "idf = []\n",
    "for i,_ in enumerate(unigrams):\n",
    "    dfi = 0\n",
    "    for j in range(TFIDF_BoW.shape[0]):\n",
    "        # on compte le nombre de documents qui contiennent ce mot\n",
    "        dfi += 1 if TFIDF_BoW[j, i] != 0 else 0\n",
    "    # on calcul et ajoute le idf à la liste des idf\n",
    "    idf.append(math.log(TFIDF_BoW.shape[0] / dfi))\n",
    "\n",
    "# on modifie la matrice\n",
    "for j in range(TFIDF_BoW.shape[0]):\n",
    "    for i, _ in enumerate(unigrams):\n",
    "        value = TFIDF_BoW[j, i] * idf[i]\n",
    "        TFIDF_BoW[j, i] = value\n",
    "print(\"\\nBag of Word TFIDF\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(TFIDF_BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matrice mot × mot et PPMI (*positive pointwise mutual information*)\n",
    "\n",
    "Vous devez calculer la métrique PPMI. Pour une matrice $m \\times n$ $X$ :\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**\tA partir des textes du corpus d’entrainement (neg *et* pos), vous devez construire une matrice de co-occurrence mot × mot $M(w,w)$ qui contient les 5000 unigrammes les plus fréquents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice_MM\n",
      "fait en : 264.3557562828064s\n",
      "[[    0. 15033. 19527. ...    87.    73.    66.]\n",
      " [15033.     0. 18600. ...    96.   109.    72.]\n",
      " [19527. 18600.     0. ...   121.   112.    93.]\n",
      " ...\n",
      " [   87.    96.   121. ...     0.     0.     0.]\n",
      " [   73.   109.   112. ...     0.     0.     0.]\n",
      " [   66.    72.    93. ...     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "# on initialise la matrice\n",
    "matrice_MM = np.zeros((len(unigrams), len(unigrams)))\n",
    "for train_type in [\"test\", \"train\"]:\n",
    "    for classification in [\"pos\", \"neg\"]:\n",
    "        for review in dictionnaire[train_type][classification]:\n",
    "            # on selectionne seulement les mots qui font parti des 5000 selectionnés\n",
    "            words_to_consider  = list(set(unigrams).intersection(dictionnaire[train_type][classification][review]['review']))\n",
    "            # on préfère manipuler les index pour ne pas à avoir à chercher dans la liste des mots\n",
    "            index_to_consider = [unigrams.index(couple_word) for couple_word in words_to_consider]\n",
    "            # on génère tout les couples possibles à partir des index des mots présents\n",
    "            permutation = itertools.permutations(index_to_consider, 2)\n",
    "            # on met à jour la matrice\n",
    "            for index_to_consider in permutation:\n",
    "                matrice_MM[index_to_consider] += 1\n",
    "\n",
    "print(\"matrice_MM\")\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(matrice_MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\tVous devez créer une fonction `calculate_PPMI` qui prend la matrice $M(w,w)$ et la transforme en une matrice $M’(w,w)$ avec les valeurs PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice_PPMI\n",
      "fait en : 35.39947843551636s\n",
      "[[0.         0.09212505 0.20809633 ... 0.         0.         0.        ]\n",
      " [0.09212505 0.         0.14769804 ... 0.         0.         0.        ]\n",
      " [0.20809633 0.14769804 0.         ... 0.00283198 0.         0.07636374]\n",
      " ...\n",
      " [0.         0.         0.00283198 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.07636374 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "debut_mm = time.time()\n",
    "def calculate_PPMI(matrice_MM):\n",
    "    colsum = np.sum(matrice_MM, axis=0)\n",
    "    row_sum = np.sum(matrice_MM, axis=1)\n",
    "    sum = np.sum(matrice_MM)\n",
    "    expected = np.zeros((matrice_MM.shape[0], matrice_MM.shape[1]))\n",
    "    ppmi =  np.zeros((matrice_MM.shape[0], matrice_MM.shape[1]))\n",
    "    for index,_ in np.ndenumerate(expected):\n",
    "        expected[index] = (row_sum[index[0]] * colsum[index[1]]) / sum\n",
    "        pmi = math.log(matrice_MM[index] / expected[index]) if matrice_MM[index] > 0 else 0\n",
    "        ppmi[index] = pmi if pmi > 0 else 0\n",
    "\n",
    "    return ppmi\n",
    "\n",
    "print(\"matrice_PPMI\")\n",
    "matrice_PPMI = calculate_PPMI(matrice_MM)\n",
    "print(\"fait en : \" + str(time.time() - debut_mm) + \"s\")\n",
    "print(matrice_PPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mesures de similarité\n",
    "\n",
    "En utilisant le module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html),  définissez des fonctions pour calculer les métriques suivantes :\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond à la longueur de la ligne droite entre deux points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Implémentez la fonction `get_euclidean_distance(v1 ,v2)` qui retourne la distance euclidienne entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_distance(v1 ,v2):\n",
    "    return distance.euclidean(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'écrit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarité cosinus* entre $u$ et $v$.\n",
    "\n",
    "**b)** Implémentez la fonction `get_cosinus_distance(v1, v2)` qui retourne la distance cosinus entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosinus_distance(v1, v2):\n",
    "    return distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Implémentez la fonction `get_most_similar_PPMI(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M’(w,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_PPMI(word, metric, n):\n",
    "    index_word = unigrams.index(word)\n",
    "    ligne_word = matrice_PPMI[index_word]\n",
    "    vector_distance = {}\n",
    "    i = 0\n",
    "    if metric == \"cosinus\":\n",
    "        for index, ligne in enumerate(matrice_PPMI):\n",
    "            vector_distance[unigrams[index]] = get_cosinus_distance(ligne_word, ligne)\n",
    "            i += 1\n",
    "    elif metric == \"euclidean\":\n",
    "        for index, ligne in enumerate(matrice_PPMI):\n",
    "            vector_distance[unigrams[index]] = get_euclidean_distance(ligne_word, ligne)\n",
    "            i += 1\n",
    "    else:\n",
    "        print(\"metric inconnue\")\n",
    "\n",
    "    counter = collections.Counter(vector_distance)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les mots les plus proches de \"bad\": \n",
      "['Custer', 'Streisand', 'Cagney', 'Bugs', 'Felix', 'Garfield', 'Karloff', 'Columbo', 'Barney', 'Frost', 'Matthau', 'Lincoln', 'Cinderella', 'Hart', 'Radio']\n",
      "\n",
      "les mots les plus proches de \"bad\": \n",
      "['beautifully', 'beautiful', 'young', 'war', 'War', 'marriage', 'emotional', 'relationship', 'lives', 'journey', 'father', 'life', 'beauty', 'mother', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_PPMI(\"bad\", \"euclidean\", 15)\n",
    "print(most_similar)\n",
    "\n",
    "print(\"\\nles mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_PPMI(\"bad\", \"cosinus\", 15)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici<-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Implémentez la fonction `get_most_similar_TFIDF(word, metric, n)` qui prend un mot en entrée et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures à tester sont : la distance euclidienne et la distance cosinus implantées ci-dessus. Le vecteur du mot word doit être extrait de la matrice $M(d,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_TFIDF(word, metric, n):\n",
    "    index_word = unigrams.index(word)\n",
    "    column_word = TFIDF_BoW[:, index_word]\n",
    "    vector_distance = {}\n",
    "    if metric == \"cosinus\":\n",
    "        for index, column in enumerate(TFIDF_BoW.T):\n",
    "            vector_distance[unigrams[index]] = get_cosinus_distance(column_word, column)\n",
    "    elif metric == \"euclidean\":\n",
    "        for index, column in enumerate(TFIDF_BoW.T):\n",
    "            vector_distance[unigrams[index]] = get_euclidean_distance(column_word, column)\n",
    "    else:\n",
    "        print(\"metric inconnue\")\n",
    "\n",
    "    counter = collections.Counter(vector_distance)\n",
    "    top_unigrams = counter.most_common(n)\n",
    "    return [couple[0] for couple in top_unigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Trouvez les 5 mots les plus similaires au mot « bad » et affichez-les, pour chacune des deux distances. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les mots les plus proches de \"bad\": \n",
      "['show', 'game', 'match', 'Batman', 'series', 'book', 'Custer', 'Bond', 'Holmes', 'war', 'THE', 'horror', 'film', 'episode', 'Tarzan']\n",
      "\n",
      "les mots les plus proches de \"bad\": \n",
      "['Felix', 'Cassavetes', 'Bugs', 'Scarlett', 'Custer', 'Matthau', 'Lemmon', 'Heart', 'Streisand', 'Garfield', 'Stanwyck', 'Bourne', 'Jesse', 'Le', 'Holly']\n"
     ]
    }
   ],
   "source": [
    "print(\"les mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_TFIDF(\"bad\", \"euclidean\", 15)\n",
    "print(most_similar)\n",
    "\n",
    "print(\"\\nles mots les plus proches de \\\"bad\\\": \")\n",
    "most_similar = get_most_similar_TFIDF(\"bad\", \"cosinus\", 15)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification de documents avec un modèle de langue\n",
    "\n",
    "En vous inspirant de [cet article](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139), entraînez deux modèles de langue $n$-gramme de caractère avec lissage de Laplace, l'un sur le corpus `pos`, l'autre sur le corpus `neg`. Puis, pour chaque document $D$, calculez sa probabilité selon vos deux modèles : $P(D \\mid \\textrm{pos})$ et $P(D \\mid \\textrm{neg})$.\n",
    "\n",
    "Vous pourrez alors prédire sa classe $\\hat{c}_D \\in (\\textrm{pos}, \\textrm{neg})$ en prenant :\n",
    "\n",
    "$$\\hat{c}_D = \\begin{cases}\n",
    "\\textrm{pos} & \\textrm{si } P(D \\mid \\textrm{pos}) > P(D \\mid \\textrm{neg}) \\\\\n",
    "\\textrm{neg} & \\textrm{sinon}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à une erreur de notre part, nous avons commencer à considérer les n_grammes de mots et non pas de caractère, nous avons préféré garder cette partie en plus car les résultats sont intéressants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple: \n",
      "P(test positif | lm_pos): -796.9957750041692\n",
      "P(test positif | lm_neg): -796.9953948232064\n",
      "P(test negatif | lm_pos): -5625.255461800974\n",
      "P(test negatif | lm_neg): -5622.482588170049\n",
      "\n",
      "sur le dictionnaire initial:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pour chaque document \"propre\" et segmenté, on créé un String qui contient tout les mots séparé d'un espace\n",
    "for type_dataset in dictionnaire:\n",
    "    for sentiment_type in dictionnaire[type_dataset]:\n",
    "        for id_review in dictionnaire[type_dataset][sentiment_type]:\n",
    "            review = dictionnaire[type_dataset][sentiment_type][id_review][\"review\"]\n",
    "            dictionnaire[type_dataset][sentiment_type][id_review][\"review_str\"] = \" \".join(review)\n",
    "\n",
    "# pour chaque document \"brut\" (un unique String) on créé une segmentation en fonction des espaces\n",
    "for type_dataset in init_dictionnaire:\n",
    "    for sentiment_type in init_dictionnaire[type_dataset]:\n",
    "        for id_review in init_dictionnaire[type_dataset][sentiment_type]:\n",
    "            review = init_dictionnaire[type_dataset][sentiment_type][id_review][\"review_str\"]\n",
    "            review = review.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./?\\|`~-=_+\"})\n",
    "            init_dictionnaire[type_dataset][sentiment_type][id_review][\"review\"] = review.split()\n",
    "\n",
    "# Ainsi, chaque dictionnaire (celui dont on a traité les documents et celui comportant les documents \"bruts\")\n",
    "# vont comporter leurs documents en une version segmentée et un version sous forme d'un grand String\n",
    "\n",
    "# Nous nous concentrons sur les n_grammes de mots en nous inspirant du code donné par l'article\n",
    "def train_words_lm(documents_lists, order=3):\n",
    "    lm = defaultdict(Counter)\n",
    "    # on compte les fréquences des mots\n",
    "    for review_index in documents_lists:\n",
    "        if \"review\" not in documents_lists[review_index]:\n",
    "            print(documents_lists[review_index])\n",
    "        review = documents_lists[review_index][\"review\"]\n",
    "        for i in range(len(review)-order):\n",
    "            history, word = review[i:i+order], review[i+order]\n",
    "            lm[tuple(history)][word]+=1\n",
    "    # cette fonction va nous servire à déterminer les probabilitées associées à chaque n-gramme\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        unsmooth = {}\n",
    "        # pour toute les probabilitées, nous nous servons du logarithme de base 10 pour éviter les erreurs d'arrondie\n",
    "        for c,cnt in counter.items():\n",
    "            # on calcule la probabilité avec la formule de lissage de laplace\n",
    "            unsmooth[c] = math.log((cnt + 1)/(s + taille_voc))\n",
    "        # on applique la cas où le mot n'existe pas\n",
    "        unsmooth[\"not_found\"] = math.log(1/(s + taille_voc))\n",
    "        return unsmooth\n",
    "    outlm = {hist:normalize(words) for hist, words in lm.items()}\n",
    "    # on applique la cas où le contexte n'existe pas\n",
    "    outlm[\"not_found\"] = math.log(1 / int(taille_voc))\n",
    "    return outlm\n",
    "\n",
    "lm_pos = train_words_lm(dictionnaire[\"train\"][\"pos\"])\n",
    "lm_neg = train_words_lm(dictionnaire[\"train\"][\"neg\"])\n",
    "\n",
    "# On s'inspire grandement du code fournis dans l'article\n",
    "def calcul_proba(document, lm, order=3):\n",
    "    proba = 0\n",
    "    for i in range(len(document) - order):\n",
    "        history, word = tuple(document[i:i+order]), document[i+order]\n",
    "        if history in lm:\n",
    "            if word in lm[history]:\n",
    "                proba += lm[history][word] \n",
    "            else:\n",
    "                proba += lm[history][\"not_found\"]\n",
    "        else:\n",
    "            proba += lm[\"not_found\"]\n",
    "    return proba\n",
    "\n",
    "print(\"exemple: \")\n",
    "print(\"P(test positif | lm_pos): \" + str(calcul_proba(dictionnaire[\"test\"][\"pos\"][10][\"review\"], lm_pos)))\n",
    "print(\"P(test positif | lm_neg): \" + str(calcul_proba(dictionnaire[\"test\"][\"pos\"][10][\"review\"], lm_neg)))\n",
    "print(\"P(test negatif | lm_pos): \" + str(calcul_proba(dictionnaire[\"test\"][\"neg\"][10][\"review\"], lm_pos)))\n",
    "print(\"P(test negatif | lm_neg): \" + str(calcul_proba(dictionnaire[\"test\"][\"neg\"][10][\"review\"], lm_neg)))\n",
    "# variables qui serviront à l'affichage\n",
    "pos = []\n",
    "neg = []\n",
    "# nombre d'ordre que nous testerons\n",
    "n = 5\n",
    "print(\"\\nsur le dictionnaire initial:\")\n",
    "for i in range(n + 1):\n",
    "    # on entraine nos deux modèles de langues\n",
    "    lm_pos = train_words_lm(init_dictionnaire[\"train\"][\"pos\"], i)\n",
    "    lm_neg = train_words_lm(init_dictionnaire[\"train\"][\"neg\"], i)\n",
    "    for classe in init_dictionnaire[\"train\"]:\n",
    "        class_by_index = []\n",
    "        nb_pos = 0\n",
    "        # on prédit pour chaque document si il est positif ou négatif\n",
    "        for index_doc in range(len(dictionnaire[\"test\"][classe])):\n",
    "            proba_pos = calcul_proba(init_dictionnaire[\"test\"][classe][index_doc][\"review\"], lm_pos, i)\n",
    "            proba_neg = calcul_proba(init_dictionnaire[\"test\"][classe][index_doc][\"review\"], lm_neg, i)\n",
    "            \n",
    "            if proba_pos > proba_neg:\n",
    "                nb_pos += 1\n",
    "                class_by_index.append(1)\n",
    "            else:\n",
    "                class_by_index.append(0)\n",
    "        \n",
    "        # on stock l'information pour l'affichage\n",
    "        if classe == \"pos\": pos.append(nb_pos / len(class_by_index))\n",
    "        else: neg.append((len(class_by_index) - nb_pos) / len(class_by_index))\n",
    "        \n",
    "plt.plot(pos, label=\"class positif\")\n",
    "plt.plot(neg, label=\"class négatif\")\n",
    "plt.xlabel('orders')\n",
    "plt.ylabel('précision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe est ici exactement le même, mais nous avons utilisé cette fois ci des n-grammes de lettres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sur le dictionnaire propre\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-108d73554f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_doc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionnaire\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mproba_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcul_proba_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionnaire\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mproba_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcul_proba_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionnaire\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mproba_pos\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mproba_neg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mnb_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-108d73554f75>\u001b[0m in \u001b[0;36mcalcul_proba_char\u001b[0;34m(document, lm, order)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mproba\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_char_lm(data, order=4):\n",
    "    lm = defaultdict(Counter)\n",
    "    voc = set()\n",
    "    for review_index in data:\n",
    "        review = data[review_index][\"review_str\"]\n",
    "        # comme dans l'article on marque le début d'un document\n",
    "        pad = \"~\" * order\n",
    "        padded_review = pad + review\n",
    "        for i in range(len(padded_review) - order):\n",
    "            history, char = padded_review[i:i + order], padded_review[i + order]\n",
    "            if char not in voc: voc.add(char)\n",
    "            lm[history][char] += 1\n",
    "\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        unsmooth = {}\n",
    "        for c, cnt in counter.items():\n",
    "            unsmooth[c] = math.log((cnt + 1) / (s + len(voc)))\n",
    "        unsmooth[\"not_found\"] = math.log(1 / (s + len(voc)))\n",
    "        return unsmooth\n",
    "\n",
    "    outlm = {hist: normalize(chars) for hist, chars in lm.items()}\n",
    "    outlm[\"not_found\"] = math.log(1 / len(voc))\n",
    "    return outlm\n",
    "\n",
    "\n",
    "def calcul_proba_char(document, lm, order=3):\n",
    "    proba = 0\n",
    "    for i in range(len(document) - order):\n",
    "        history, word = document[i:i + order], document[i + order]\n",
    "        if history in lm:\n",
    "            if word in lm[history]:\n",
    "                proba += lm[history][word]\n",
    "            else:\n",
    "                proba += lm[history][\"not_found\"]\n",
    "        else:\n",
    "            proba += lm[\"not_found\"]\n",
    "    return proba\n",
    "# on crée un dictionnaire pour stocker nos prédictions\n",
    "pred = {\n",
    "    \"clean\" : { \"pos\": {}, \n",
    "              \"neg\": {}},\n",
    "    \"raw\": { \"pos\": {}, \n",
    "              \"neg\": {}}\n",
    "       }\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "print(\"sur le dictionnaire propre\")\n",
    "n = 10\n",
    "for i in range(3, n):\n",
    "    lm_pos = train_char_lm(dictionnaire[\"train\"][\"pos\"], i)\n",
    "    lm_neg = train_char_lm(dictionnaire[\"train\"][\"neg\"], i)\n",
    "    for classe in init_dictionnaire[\"train\"]:\n",
    "        pred[\"clean\"][classe][i] = []\n",
    "        class_by_index = []\n",
    "        nb_pos = 0\n",
    "        for index_doc in range(len(dictionnaire[\"test\"][\"pos\"])):\n",
    "            proba_pos = calcul_proba_char(dictionnaire[\"test\"][classe][index_doc][\"review_str\"], lm_pos, i)\n",
    "            proba_neg = calcul_proba_char(dictionnaire[\"test\"][classe][index_doc][\"review_str\"], lm_neg, i)\n",
    "            if proba_pos > proba_neg:\n",
    "                nb_pos += 1\n",
    "                pred[\"clean\"][classe][i].append(1)\n",
    "            else:\n",
    "                pred[\"clean\"][classe][i].append(0)\n",
    "\n",
    "        if classe == \"pos\":\n",
    "            pos.append(nb_pos / len(dictionnaire[\"test\"][\"pos\"]))\n",
    "        else: \n",
    "            neg.append((len(dictionnaire[\"test\"][\"pos\"]) - nb_pos) / len(dictionnaire[\"test\"][\"pos\"]))\n",
    "\n",
    "plt.plot(list(range(3,n)),pos, label=\"class positif\")\n",
    "plt.plot(list(range(3,n)),neg, label=\"class négatif\")\n",
    "plt.xlabel('orders')\n",
    "plt.ylabel('précision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "print(\"\\nsur le dictionnaire sans modification:\")\n",
    "for i in range(3, n):\n",
    "    class_by_index = []\n",
    "    nb_pos = 0\n",
    "    lm_pos = train_char_lm(init_dictionnaire[\"train\"][\"pos\"], i)\n",
    "    lm_neg = train_char_lm(init_dictionnaire[\"train\"][\"neg\"], i)\n",
    "    for classe in init_dictionnaire[\"train\"]:\n",
    "        pred[\"raw\"][classe][i] = []\n",
    "        class_by_index = []\n",
    "        nb_pos = 0\n",
    "        for index_doc in range(len(dictionnaire[\"test\"][classe])):\n",
    "            proba_pos = calcul_proba_char(init_dictionnaire[\"test\"][classe][index_doc][\"review_str\"], lm_pos, i)\n",
    "            proba_neg = calcul_proba_char(init_dictionnaire[\"test\"][classe][index_doc][\"review_str\"], lm_neg, i)\n",
    "            if proba_pos > proba_neg:\n",
    "                nb_pos += 1\n",
    "                pred[\"raw\"][classe][i].append(1)\n",
    "            else:\n",
    "                pred[\"raw\"][classe][i].append(0)\n",
    "\n",
    "        if classe == \"pos\":\n",
    "            pos.append(nb_pos / len(dictionnaire[\"test\"][\"pos\"]))\n",
    "        else:\n",
    "            neg.append((len(dictionnaire[\"test\"][\"pos\"]) - nb_pos) / len(dictionnaire[\"test\"][\"pos\"]))        \n",
    "\n",
    "plt.plot(list(range(3,n)), pos, label=\"class positif\")\n",
    "plt.plot(list(range(3,n)),neg, label=\"class négatif\")\n",
    "plt.xlabel('orders')\n",
    "plt.ylabel('précision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification de documents avec sac de mots et Naive Bayes\n",
    "\n",
    "Ici, vous utiliserez l'algorithme Multinomial Naive Bayes (disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) pour classifier les documents. Vous utiliserez un modèle sac de mots (en anglais *bag of words*, ou BoW) avec TF-IDF pour représenter vos documents.\n",
    "\n",
    "*Note :* vous avez déjà construit la matrice TF-IDF à la section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# y = 0:pos 1:neg\n",
    "X_test = TFIDF_BoW[0:25000][:]\n",
    "y_test_nb = np.full((1, 12500), 0)\n",
    "y_test_nb = np.append(y_test_nb, np.full((1, 12500), 1))\n",
    "\n",
    "X_train = TFIDF_BoW[25000:][:]\n",
    "y_train = np.full((1, 12500), 0)\n",
    "y_train = np.append(y_train, np.full((1, 12500), 1))\n",
    "            \n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_nb = classifier.predict(X_test)\n",
    "print(y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Améliorations\n",
    "\n",
    "Ici, vous devez proposer une méthode d'amélioration pour le modèle précédent, la justifier et l'implémenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Écrivez vos explications ici <-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation\n",
    "\n",
    "Évaluation des modèles des sections 4, 5, 6 sur les données de test. On attend les métriques suivantes : *accuracy*, et pour chaque classe précision, rappel, score F1. Vous pourrez utiliser le module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy NB:\")\n",
    "print(accuracy_score(y_test_nb, y_pred_nb))\n",
    "\n",
    "# precision\n",
    "print(\"Precision NB:\")\n",
    "print(precision_score(y_test_nb, y_pred_nb, average='binary'))\n",
    "\n",
    "# recall\n",
    "print(\"Recall NB:\")\n",
    "print(recall_score(y_test_nb, y_pred_nb, average='binary'))\n",
    "\n",
    "# F1-score\n",
    "print(\"F1-score NB:\")\n",
    "print(f1_score(y_test_nb, y_pred_nb, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentez vos résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-> Commentez ici vos résultats <-*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
